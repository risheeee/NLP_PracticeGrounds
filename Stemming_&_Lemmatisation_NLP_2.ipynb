{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "**Stemming** is a fundamental technique in natural language processing (NLP) that aims to reduce words to their root or base form. This process involves removing affixes from words to normalize them and improve text analysis and retrieval tasks. Stemming algorithms play a crucial role in various NLP applications:\n",
        "* information retrieval\n",
        "* sentiment analysis\n",
        "* text mining.\n",
        "\n",
        "There are a quite few many types of stemming, the ones we'll be checking out are:\n",
        "1. Porter Stemmer\n",
        "2. Snowball Stemmer\n",
        "3. Lancaster Stemmer\n",
        "4. RegxpStemmer"
      ],
      "metadata": {
        "id": "lte8Z2qwduW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Porter Stemmer"
      ],
      "metadata": {
        "id": "dF9sF46keZMG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tMw4lLmqS9t_"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['run', 'runner', 'running', 'swim', 'swimmer', 'swimming', 'eat', 'eaten', 'eating', 'bouncer', 'bounce', 'bouncing']"
      ],
      "metadata": {
        "id": "hSABorzXegvN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [PorterStemmer().stem(word) for word in words]\n",
        "for original, stemmed in zip(words, stemmed_words):\n",
        "  print(f'{original} -----> {stemmed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mchYrqzbev_j",
        "outputId": "02331912-4311-4f50-edd8-a911ba3e27a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run -----> run\n",
            "runner -----> runner\n",
            "running -----> run\n",
            "swim -----> swim\n",
            "swimmer -----> swimmer\n",
            "swimming -----> swim\n",
            "eat -----> eat\n",
            "eaten -----> eaten\n",
            "eating -----> eat\n",
            "bouncer -----> bouncer\n",
            "bounce -----> bounc\n",
            "bouncing -----> bounc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, we can see that many words are successfully being stemmed down to their root words. However many aren't. This is oone problem with porter stemmer as it does not give the guarantee that all the words would get stemmed down"
      ],
      "metadata": {
        "id": "fQBFJpqpfXI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Snowball stemmer"
      ],
      "metadata": {
        "id": "44nSfZbyfqfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "ufi544-WfLKM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [SnowballStemmer('english').stem(word) for word in words]\n",
        "for original, stemmed in zip(words, stemmed_words):\n",
        "  print(f'{original} -----> {stemmed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PIalTKofwVq",
        "outputId": "f26358f1-df4e-4a8a-f891-853f28ddfbf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run -----> run\n",
            "runner -----> runner\n",
            "running -----> run\n",
            "swim -----> swim\n",
            "swimmer -----> swimmer\n",
            "swimming -----> swim\n",
            "eat -----> eat\n",
            "eaten -----> eaten\n",
            "eating -----> eat\n",
            "bouncer -----> bouncer\n",
            "bounce -----> bounc\n",
            "bouncing -----> bounc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the same problem with Snowball stemmer however in lower quantity"
      ],
      "metadata": {
        "id": "x7ur38eVgGeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lancaster Stemmer"
      ],
      "metadata": {
        "id": "qMLDKWNkgMMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "id": "i4ij6JnCf9JH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [LancasterStemmer().stem(word) for word in words]\n",
        "for original, stemmed in zip(words, stemmed_words):\n",
        "  print(f'{original} -----> {stemmed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-PPHcXKgQsb",
        "outputId": "cda6763c-d363-461d-aaaa-e55ba005e21c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run -----> run\n",
            "runner -----> run\n",
            "running -----> run\n",
            "swim -----> swim\n",
            "swimmer -----> swim\n",
            "swimming -----> swim\n",
            "eat -----> eat\n",
            "eaten -----> eat\n",
            "eating -----> eat\n",
            "bouncer -----> bount\n",
            "bounce -----> bount\n",
            "bouncing -----> bount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Damn! Lancaster perfomed better than the above two, however it still struggles with 'bounce' family"
      ],
      "metadata": {
        "id": "QM2u6L0jgeRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regxpstemmer\n",
        "A stemmer that uses regular expressions to identify morphological affixes. Any substrings that match the regular expressions will be removed."
      ],
      "metadata": {
        "id": "sUJv_rEngnol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "T3V1WngOgYSF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [RegexpStemmer('ing$|s$|e$|able$').stem(word) for word in words]\n",
        "for original, stemmed in zip(words, stemmed_words):\n",
        "  print(f'{original} -----> {stemmed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZfUJiWDg1fY",
        "outputId": "7935f952-a1d0-447c-ee76-730b62ffcf88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run -----> run\n",
            "runner -----> runner\n",
            "running -----> runn\n",
            "swim -----> swim\n",
            "swimmer -----> swimmer\n",
            "swimming -----> swimm\n",
            "eat -----> eat\n",
            "eaten -----> eaten\n",
            "eating -----> eat\n",
            "bouncer -----> bouncer\n",
            "bounce -----> bounc\n",
            "bouncing -----> bounc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the Regxpstemmer performs the worst, that's why its not recommended to use by many experts"
      ],
      "metadata": {
        "id": "lBftOBrthZnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "**Lemmatization** is a more advanced and accurate form of text normalization compared to stemming. Instead of simply chopping off word endings, lemmatization considers the wordâ€™s meaning and part of speech to reduce it to its base or root form `(lemma)`, ensuring the word retains its meaning.\n",
        "For example it converts:\n",
        "* Running --> Run\n",
        "* better -> good\n",
        "\n",
        "unlike stemming, which incorrectly converts `Running --> Runn` as we already saw above\n",
        "\n",
        "NLTK provides a lemmatization module called as `WordNetLemmatizer`, which uses the WordNet lexical database to find the correct base forms of words. You can specify the part of speech (POS) to get more accurate results.\n",
        "\n",
        "the parts of speech (POS) are:\n",
        "1. `n` - nouns\n",
        "2. `v` - verbs\n",
        "3. `a` - adjectives\n",
        "4. `r` - adverbs"
      ],
      "metadata": {
        "id": "oNIm2cM7tDfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JASjRFZWhGTu",
        "outputId": "59ac2585-b484-404c-ec12-7195b911811f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see an example for it  "
      ],
      "metadata": {
        "id": "M84vx7rEu93x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WordNetLemmatizer().lemmatize('going')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "750EEsM5uq-Q",
        "outputId": "55535f17-a7b4-4506-bdee-c745d8da1b51"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¤” well.. according to everything i had already said, 'going' should have been converted to 'go' right?\n",
        "\n",
        "Wrong, the defauls POS in WordNet is `n` but as we all may have studied in our schools, going..is an action word and therfore it is a verb. So lets try keeping the POS to `v` and see the results"
      ],
      "metadata": {
        "id": "ppPdyvXwvMTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WordNetLemmatizer().lemmatize('going', pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n15oOyxwvrCd",
        "outputId": "7760fd41-e83f-4eed-969b-a143dd3d5671"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yep, as expected!"
      ],
      "metadata": {
        "id": "KdLnh1N9vuAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
        "for original, lemmatized in zip(words, lemmatized_words):\n",
        "  print(f'{original} -----> {lemmatized}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LePhKezxuhqV",
        "outputId": "fc6116d7-58b8-4888-ebb8-4d266a669aa9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run -----> run\n",
            "runner -----> runner\n",
            "running -----> run\n",
            "swim -----> swim\n",
            "swimmer -----> swimmer\n",
            "swimming -----> swim\n",
            "eat -----> eat\n",
            "eaten -----> eat\n",
            "eating -----> eat\n",
            "bouncer -----> bouncer\n",
            "bounce -----> bounce\n",
            "bouncing -----> bounce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "woah, it correctly lemmatized all the words present here..except a few that's because the words `Runner, Swimmer and Bouncer` are nouns and not a verb"
      ],
      "metadata": {
        "id": "nb3FHu9ewGfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in ['runner', 'swimmer', 'bouncer']]\n",
        "for original, lemmatized in zip(['runner', 'swimmer', 'bouncer'], lemmatized_words):\n",
        "  print(f'{original} -----> {lemmatized}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-0WzAZEv6kR",
        "outputId": "95b3b177-1516-482b-e7d4-993bdd0d89b4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runner -----> runner\n",
            "swimmer -----> swimmer\n",
            "bouncer -----> bouncer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ˜¯They should have been converted to their root forms but still havent!\n",
        "\n",
        "reason? The WordNetLemmatizer didn't convert `runner`, `swimmer`, and `bouncer` to their root verbs because it treats these words as distinct nouns rather than verb derivatives. WordNet relies on its predefined lexicon and doesn't automatically strip suffixes like `-er`\n",
        "\n"
      ],
      "metadata": {
        "id": "7AKLDz_XxhIK"
      }
    }
  ]
}