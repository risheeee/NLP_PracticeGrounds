{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** is a fundamental step in Natural Language Processing (NLP) that involves dividing text into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the specific requirements of the task at hand. Effective tokenization is crucial as it lays the groundwork for subsequent NLP tasks such as parsing, part-of-speech tagging, and machine translation."
      ],
      "metadata": {
        "id": "pbqgRNjybKiN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi5pFGo8LtiZ"
      },
      "outputs": [],
      "source": [
        "!pip install nltk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "There is a cat! the cat finds a box.\n",
        "the cat gets in the box.\n",
        "the cat sleeps.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sCzVue7BMYLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwUDm3PbMnJV",
        "outputId": "7797d456-74c2-4e90-8f9f-63e9a58b3072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "There is a cat! the cat finds a box.\n",
            "the cat gets in the box. \n",
            "the cat sleeps.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50VwjvrAMoH7",
        "outputId": "90a616eb-3853-4944-ac5c-44b7f25d403e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS7tqf1DMvqy",
        "outputId": "09df07d6-876f-4f49-98f2-5dabdc3bac3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nThere is a cat!',\n",
              " 'the cat finds a box.',\n",
              " 'the cat gets in the box.',\n",
              " 'the cat sleeps.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq9G4mpDMxdM",
        "outputId": "64136c4b-9263-46db-bb46-b4a371ea5749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cat',\n",
              " '!',\n",
              " 'the',\n",
              " 'cat',\n",
              " 'finds',\n",
              " 'a',\n",
              " 'box',\n",
              " '.',\n",
              " 'the',\n",
              " 'cat',\n",
              " 'gets',\n",
              " 'in',\n",
              " 'the',\n",
              " 'box',\n",
              " '.',\n",
              " 'the',\n",
              " 'cat',\n",
              " 'sleeps',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whpD_ES8NNOi",
        "outputId": "12cfa1ef-01e5-4486-ad92-b2b21000b9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cat',\n",
              " '!',\n",
              " 'the',\n",
              " 'cat',\n",
              " 'finds',\n",
              " 'a',\n",
              " 'box.',\n",
              " 'the',\n",
              " 'cat',\n",
              " 'gets',\n",
              " 'in',\n",
              " 'the',\n",
              " 'box.',\n",
              " 'the',\n",
              " 'cat',\n",
              " 'sleeps',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In TreeBankTokenizer fullstop is not treated as a seperate, except the last full stop of a corpus"
      ],
      "metadata": {
        "id": "SqaoK8IxNz0C"
      }
    }
  ]
}